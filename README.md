# Few-Shot Classification for Hate Speech Detection

This repository is dedicated to showcasing the practical applications explored in the research "Few-Shot Classification for Hate Speech Detection". The primary motivation behind this study was to assess and validate the effectiveness of the Task-Aware Representation of Sentences (TARS) methodology in the context of hate speech detection across multiple languages, using the XLM-RoBERTa model.

## Research Objectives

The research was driven by the following objectives:

1. **Evaluating TARS on Diverse Hate Speech Corpora**: To evaluate the effectiveness of TARS in handling semantically diverse and unbalanced hate speech datasets known for their complexity.

2. **Few-Shot and Zero-Shot Learning**: To explore the TARS framework's capacity for few-shot learning—making accurate predictions with limited labeled data—and zero-shot learning, inferring classifications for previously unseen labels.

3. **Knowledge Retention in TARS**: To investigate how well the TARS approach retains previously learned information when incorporating new labels and additional labeled data.

4. **TARS Performance Across Languages**: To assess TARS performance on hate speech datasets in various languages, including English, German, and Portuguese, thus exploring the method's multilingual capabilities.

## Repository Overview

Contained within are Jupyter notebooks (`model_training_final_step.ipynb` and `EVAL_F1_all_datasets.ipynb`) that exemplify the TARS method's implementation for hate speech detection. These are intended as illustrative samples to accompany the study's findings.

## Insights and Demonstrations

The notebooks provide insight into the TARS methodology's practical application, showcasing the potential for addressing hate speech detection challenges in a low-resource setting.

## Contact

For any inquiries or further discussion related to the research, please reach out via the contact information provided within this repository.

## License

This research and the accompanying demonstrations are provided under the MIT License.
